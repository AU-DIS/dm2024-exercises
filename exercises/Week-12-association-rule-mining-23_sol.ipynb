{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Week 10 - Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "from utilities.load_data import load_market_basket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "\n",
    "Consider a dataset of transactions $D$, and let $D'$ be a dataset derived from $D$ by independently deleting items from transactions in $D$. In particular, any item in any transaction in $D$ is deleted with probability $p$.\n",
    "\n",
    "1. Given an itemset $S$, compute its expected support in $D'$ as a function of its support in $D$.\n",
    "2. Compute the probability that an itemset $S$, which is frequent in $D$, is also frequent in $D'$, under the same minimum support threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions\n",
    "1.  To solve this, we need to think of what the probability is for loosing each \"supporting\" transaction.\n",
    "    Suppose that $|S| = k$ and consider a transaction $t_i$ of which $S \\subseteq i(t_i)$, then since each item in $t_i$ is deleted independently with probability $p$, the probability of keeping the support from $t_i$ is $Pr[\\text{not deleting any item } x\\in S \\text{ from }t_i] = \\prod_{x \\in S} (1-p) = (1-p)^k$.  \n",
    "    Thus computing the expected value of the support for $S$ in $D'$ amounts to\n",
    "    $$\n",
    "        \\mathbb{E}\\left[Sup(S, D')\\right] = \\sum_{t_i \\in D} (1-p)^k 1 = (1-p)^k Sup(S, D)\n",
    "    $$\n",
    "    \n",
    "2.  To compute this probability, we again consider an itemset $S$ with size $k$, i.e., $|S| = k$.\n",
    "    Again, we can realize that a transaction $t_i$ is going to be supportive to $S$ if all $x \\in S$ of $t_i$ are kept. This has probability $\\hat p = (1-p)^k$.\n",
    "    We can think of $\\hat p$ as a bernouli distribution.  \n",
    "    \n",
    "    Now we need to quantify the probability of keeping at least the minimum support ($m$) of the supporting transactions. \n",
    "    Each of the $n := Sup(S, D)$ supporting transactions will be kept with probability $\\hat p$ so we can think of this setup as a binomial distribution with $n$ trials.\n",
    "    The binomial distribution has probability mass function $Pr[X = k] = {n \\choose k }\\hat p^k(1-\\hat p)^{n-k}$ for bernouli trials with $n$ samples (transactions in this case) and exactly $k$ positive outcomes.\n",
    "    \n",
    "    Now we just need to sum up the probabilities of all $k \\geq m$.\n",
    "    \n",
    "    $$\n",
    "        Pr[sup(S, D') \\geq m] = \\sum_{k=m}^{n} {n \\choose k}\\hat p^k (1-\\hat p)^{n - k}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.\n",
    "\n",
    "The work you do in this exercise will be useful also in your hand-in.\n",
    "\n",
    "We learned the Apriori algorithm in class. Make your own implementation. \n",
    "\n",
    "We will use the anonymized real-world retail market basket data from: http://fimi.ua.ac.be/data/.\n",
    "\n",
    "This data comes from an anonymous Belgian retail store, and was donated by Tom Brijs from Limburgs Universitair Centrum, Belgium. The original data contains 16,470 different items and 88,162 transactions. You may only work with the top-50 items in terms of occurrence frequency.\n",
    "\n",
    "Your task is to:\n",
    "1. Implement the Apriori algorithm.\n",
    "2. Apply the Apriori algorithm on these data to find association rules with minimum support 0.05 and minimum confidence 0.7. Write down the rules in descending order of confidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "### TODO Your code here\n",
    "def compute_candidates(prev_itemset):\n",
    "    Ck = set()\n",
    "    # Join step\n",
    "    for itemset in prev_itemset:\n",
    "        its1 = tuple(sorted(itemset))\n",
    "        for itemset2 in prev_itemset:\n",
    "            its2 = tuple(sorted(itemset2))\n",
    "            if its1[:-1] == its2[:-1]:\n",
    "                if its1[-1] < its2[-1]: Ck.add(its1 + its2[-1:])\n",
    "\n",
    "    # Pruning step\n",
    "    to_remove = set()\n",
    "    for c in Ck:\n",
    "        for subset in combinations(c, len(c)-1):\n",
    "            if not subset in prev_itemset:\n",
    "                to_remove.add(c)\n",
    "                break\n",
    "    for c in to_remove:\n",
    "        Ck.remove(c)\n",
    "    \n",
    "    return Ck\n",
    "\n",
    "\n",
    "def apriori_algorithm_paper(T, support=0.05, min_confidence=0.7):\n",
    "    # We are going to keep dictionaries of dictionaries:\n",
    "    # itemsets: Dict(int, Dict(tuple, int)) where the outer int is the size of the itemset, \n",
    "    # the tuple is the itemset and the last int is the count of that itemset \n",
    "    \n",
    "    itemsets = dict()\n",
    "    \n",
    "    # 1. Count size one itemsets\n",
    "    num_transactions = len(T)\n",
    "    \n",
    "    counts = dict()\n",
    "    for t in T:\n",
    "        for i in t:\n",
    "            k = (i,)\n",
    "            if k in counts: counts[k] += 1\n",
    "            else:           counts[k]  = 1\n",
    "    \n",
    "    itemsets[1] = {\n",
    "        i: c for (i, c) in counts.items() if (c / num_transactions) >= support\n",
    "    }\n",
    "    \n",
    "    still_applicable = [True] * len(T) # Remember which rows are still candidates for matching itemsets (for efficiency)\n",
    "    \n",
    "    # 2. Construct larger itemsets\n",
    "    k = 2\n",
    "    while k-1 in itemsets and itemsets[k-1]:\n",
    "        prev_is = set(itemsets[k-1].keys())\n",
    "        \n",
    "        Ck = compute_candidates(prev_is)\n",
    "        \n",
    "        # Count occurences of candidate sets in the transaction database\n",
    "        counts = dict()\n",
    "        for row, t in enumerate(T): \n",
    "            if not still_applicable[row]: continue\n",
    "            \n",
    "            found_any = False\n",
    "            for c in Ck:\n",
    "                if set.issubset(set(c), t):\n",
    "                    if c in counts: counts[c] += 1\n",
    "                    else:           counts[c] = 1\n",
    "                    found_any = True\n",
    "            \n",
    "            still_applicable[row] = found_any\n",
    "            \n",
    "        # Keep candidate sets with proper support\n",
    "        itemsets[k] = {\n",
    "            c: counts[c] for c in Ck if (counts[c] / num_transactions) >= support\n",
    "        }\n",
    "        k += 1\n",
    "    \n",
    "    # 3. generate rules from itemsets\n",
    "    count = lambda i: itemsets[len(i)][i]                      # For easier look up of counts\n",
    "    rule  = lambda lhs, rhs: \"%s => %s\" % (str(lhs), str(rhs)) # For generating rule strings\n",
    "    rules = []                                                 # Collection of rules\n",
    "    \n",
    "    def apriori_rule_generation(itemset, Hm):\n",
    "        \"\"\"\n",
    "            Recursive formulation for generating rules. Starting from itemsets of size 2.\n",
    "        \"\"\"\n",
    "        if (len(itemset)-1) <= len(Hm[0]): return # Stop recursion if itemset is only one larger than candidates\n",
    "\n",
    "        Hm = list(compute_candidates(Hm))         # Same computation as for building itemsets\n",
    "        to_remove = []\n",
    "        \n",
    "        confidence = lambda lhs: count(itemset) / count(lhs)\n",
    "\n",
    "        for hm in Hm:\n",
    "            lhs = tuple(sorted(set(itemset).difference(set(hm))))\n",
    "            \n",
    "            conf = confidence(lhs)\n",
    "            if conf >= min_confidence:\n",
    "                rules.append((rule(lhs, hm), conf))\n",
    "            else:\n",
    "                to_remove.append(hm)\n",
    "\n",
    "        for hm in to_remove:\n",
    "            Hm.remove(hm)\n",
    "         \n",
    "        if Hm: apriori_rule_generation(itemset, Hm)\n",
    "    \n",
    "    # Generate rules for all k>=2 itemsets \n",
    "    for size in itemsets.keys():\n",
    "        if size < 2: continue\n",
    "            \n",
    "        # For every itemset of this size\n",
    "        for itemset in itemsets[size].keys():\n",
    "            confidence = lambda lhs: count(itemset) / count(lhs)\n",
    "            \n",
    "            # Capture rules {others} -> {a single item}\n",
    "            for rhs in combinations(itemset, 1):\n",
    "                # Compute the left hand side\n",
    "                remaining = set(itemset).difference(set(rhs))\n",
    "                lhs = tuple(sorted(remaining))\n",
    "\n",
    "                # If the confidence is high enough, yield the rule\n",
    "                conf = confidence(lhs)\n",
    "                \n",
    "                if conf >= min_confidence:\n",
    "                    rules.append((rule(lhs, rhs), conf))\n",
    "            \n",
    "            # Recurse on H_m, where m > 1\n",
    "            H1 = list(combinations(itemset, 1))\n",
    "            apriori_rule_generation(itemset, H1)\n",
    "            \n",
    "    return itemsets, rules\n",
    "### TODO Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "rule  = lambda lhs, rhs: \"%s => %s\" % (str(lhs), str(rhs)) # For generating rule strings\n",
    "\n",
    "def compute_support(Ck, T, still_applicable=None):\n",
    "    if still_applicable is None: still_applicable = [True] * len(T)\n",
    "    \n",
    "    counts = {}\n",
    "    for row, t in enumerate(T): \n",
    "        if not still_applicable[row]: continue\n",
    "\n",
    "        found_any = False\n",
    "        for c in Ck:\n",
    "            if set.issubset(set(c), t):\n",
    "                if c in counts: counts[c] += 1\n",
    "                else:           counts[c] = 1\n",
    "                found_any = True\n",
    "\n",
    "        still_applicable[row] = found_any            \n",
    "    return counts\n",
    "\n",
    "def extend_prefix_tree(Ck_prev):\n",
    "    Ck = set()\n",
    "    # Join step\n",
    "    for itemset in Ck_prev:\n",
    "        its1 = tuple(sorted(itemset))\n",
    "        for itemset2 in Ck_prev:\n",
    "            its2 = tuple(sorted(itemset2))\n",
    "            if its1[:-1] == its2[:-1] and its1[-1] < its2[-1]: Ck.add(its1 + its2[-1:])\n",
    "\n",
    "    # Pruning step\n",
    "    to_remove = set()\n",
    "    for c in Ck:\n",
    "        for subset in combinations(c, len(c)-1):\n",
    "            if not subset in Ck_prev:\n",
    "                to_remove.add(c)\n",
    "                break\n",
    "    for c in to_remove:\n",
    "        Ck.remove(c)\n",
    "    return Ck\n",
    "\n",
    "def powerset(iterable):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s)))\n",
    "\n",
    "def apriori_algorithm_book(T, support=0.5, min_confidence=0.7):\n",
    "    n = len(T)\n",
    "    \n",
    "    # Compute Itemsets\n",
    "    itemsets = {}\n",
    "    C1 = set()\n",
    "    for t in T:\n",
    "        for ti in t: C1.add((ti,))\n",
    "    \n",
    "    still_applicable = [True] * n\n",
    "    Ck = C1\n",
    "    k = 1\n",
    "    while Ck:\n",
    "        itemsets[k] = compute_support(Ck, T, still_applicable)\n",
    "        Ck_copy = Ck.copy()\n",
    "        for itemset in Ck:\n",
    "            if itemsets[k][itemset] / n < support:\n",
    "                del itemsets[k][itemset]\n",
    "                Ck_copy.remove(itemset)\n",
    "         \n",
    "        Ck = extend_prefix_tree(Ck_copy)\n",
    "        k += 1\n",
    "     \n",
    "    # Construct Rules\n",
    "    k = 2\n",
    "    rules = []\n",
    "    while k <= max(itemsets.keys()):\n",
    "        for itemset in itemsets[k].keys():\n",
    "            for rhs in powerset(itemset):\n",
    "                remaining = set(itemset).difference(set(rhs))\n",
    "                lhs = tuple(sorted(remaining))\n",
    "                \n",
    "                conf = itemsets[k][itemset] / itemsets[len(lhs)][lhs]\n",
    "                if conf >= min_confidence:\n",
    "                    rules.append((rule(lhs, rhs), conf))\n",
    "        k+= 1\n",
    "    return itemsets, rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conf.    \t Rule\n",
      " 0.8168% \t (41, 48) => (39,)\n",
      " 0.7681% \t (38, 48) => (39,)\n",
      " 0.7637% \t (41,) => (39,)\n"
     ]
    }
   ],
   "source": [
    "# Load the retail data\n",
    "transactions = load_market_basket()\n",
    "\n",
    "def book_example():\n",
    "    return [\n",
    "        [1, 2, 4, 5],\n",
    "        [2, 3, 5],\n",
    "        [1, 2, 4, 5],\n",
    "        [1, 2, 3, 5],\n",
    "        [1, 2, 3, 4, 5], \n",
    "        [2, 3, 4],\n",
    "    ]\n",
    "    \n",
    "def filter_transactions(T, k=50):\n",
    "    \"\"\"\n",
    "        Keep only the top k items in the transactions.\n",
    "        Remove transactions that become empty.\n",
    "    \"\"\"\n",
    "    # Count occurences of each item\n",
    "    counts = [0] * 16470\n",
    "    for t in T:\n",
    "        for i in t:\n",
    "            counts[i] += 1\n",
    "\n",
    "    # Sort and select top k\n",
    "    counts = np.array(counts)\n",
    "    order  = np.argsort(counts)[::-1] # reverse the sorted order\n",
    "\n",
    "    indexes_to_keep = order[:k]       # Keep the top k items\n",
    "    index_set = set(indexes_to_keep)  # Convert to python set for efficiency\n",
    "\n",
    "    # Filter transactions\n",
    "    T_new = [t_ for t_ in  [list(filter(lambda i: i in index_set, t)) for t in T]  if t_]\n",
    "    return T_new\n",
    "\n",
    "T = filter_transactions(transactions, k=100)\n",
    "\n",
    "# Example 8.1 from the book\n",
    "# T = book_example()\n",
    "\n",
    "book = True\n",
    "if book:\n",
    "    apriori_algorithm = apriori_algorithm_book\n",
    "else:\n",
    "    apriori_algorithm = apriori_algorithm_paper\n",
    "\n",
    "itemsets, rules = apriori_algorithm(T, support=0.05, min_confidence=0.7)\n",
    "rules = sorted(rules, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"%-8s \\t %s\" % (\"Conf.\", \"Rule\"))\n",
    "for r in rules:\n",
    "    print(\"%7.4f%% \\t %s\" % r[::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.\n",
    "\n",
    "We have learned how to mine frequent itemsets and association rules from a transaction database where each transaction consists of a simple set of items. You are asked to propose a framework for mining association rules from transaction data, in which each item in a transaction is associated with an integer number that counts how many times the items appears in the transaction. In a market basket, this count number indicates the number of copies of a product in a customer’s basket. For example, we do not only care whether a customer bought fish or not, but how many pieces of fish they bought. You need to:\n",
    "\n",
    "1. Define (extend) the notion of an itemset and an association rule in the case of such data.\n",
    "\n",
    "2. Describe an efficient algorithm that mines itemsets and association rules as defined in (1). Illustrate the pruning strategies used in your algorithm and explain how they relate to the Apriori principle.\n",
    "\n",
    "3. Extend your implementation of the Apriori algorithm to handle this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "1. \n",
    "    1. The simple solution would be to let the \"identifiers\" be the item and the count. This would give rules like [(A, 2), (C, 3) => (D, 1)].\n",
    "    2. One could also keep the original formulation of an itemset but change the count by counting as many supports as the min count of the items in each transaction. This woul\n",
    "    3. Finally, one could compute mean counts in supporting transactions, i.e., [A:2, B:3.5, D:1.2] and if I then make a rule say \"E => ABD\", We could then give an additional information on how many items one could expect.\n",
    "2. For the third (and more interesting) solution above, mining would be roughly the same as the original algorithm. The differrence would be the additional book keeping for keeping the means. Note that we can compute a running mean.\n",
    "3. Alongside with the count variables, we will keep sum variables.\n",
    "    When producing rules, we will output sum / count along with confidence in order to indicate how many of each items one can expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO your code here\n",
    "# You can copy pase the code from above and adjust it\n",
    "### TODO your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional\n",
    "Test of expectation computations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "itemsets, _ = apriori_algorithm(T)\n",
    "\n",
    "p = 0.1\n",
    "\n",
    "def sample_itemset(p=0.1):\n",
    "    T_ = []\n",
    "    for t in T: \n",
    "        rs = np.random.rand(len(t))\n",
    "        new = []\n",
    "        for x, pr in zip(t, rs):\n",
    "            if pr < p: continue\n",
    "            else:      new.append(x)\n",
    "        T_.append(new)\n",
    "    itemsets, _ = apriori_algorithm(T_)\n",
    "    return itemsets\n",
    "\n",
    "\n",
    "I2 = [sample_itemset() for _ in tqdm(range(10))]\n",
    "I += I2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _ - - - - -   1  - - - -- - \n",
      "(32,) \t 15167      13650      13635.63   30   \n",
      "(38,) \t 15596      14036      14031.97   30   \n",
      "(39,) \t 50675      45607      45622.53   30   \n",
      "(41,) \t 14945      13450      13450.37   30   \n",
      "(48,) \t 42135      37921      37915.73   30   \n",
      "(65,) \t 4472       4024       4023.66    29   \n",
      " _ - - - - -   2  - - - -- - \n",
      "(38, 39) \t 10345      8379       8378.83    30   \n",
      "(32, 39) \t 8455       6848       6839.50    30   \n",
      "(41, 48) \t 9018       7304       7310.60    30   \n",
      "(32, 48) \t 8034       6507       6496.17    30   \n",
      "(39, 48) \t 29142      23605      23606.07   30   \n",
      "(38, 48) \t 7944       6434       6424.03    30   \n",
      "(39, 41) \t 11414      9245       9246.13    30   \n",
      " _ - - - - -   3  - - - -- - \n",
      "(32, 39, 48) \t 5402       3938       nan        0    \n",
      "(38, 39, 48) \t 6102       4448       4439.00    30   \n",
      "(39, 41, 48) \t 7366       5369       5372.37    30   \n"
     ]
    }
   ],
   "source": [
    "for k in range(1, max(itemsets.keys())):\n",
    "    print(\" _ - - - - -  \", k, \" - - - -- - \")\n",
    "    for key in itemsets[k].keys():\n",
    "        counts = []\n",
    "        for itemsets2 in I:\n",
    "            if key in itemsets2[k]: counts.append(itemsets2[k][key])\n",
    "        print(\"%s \\t %-10d %-10d %-10.2f %-5d\" % (str(key), itemsets[k][key], itemsets[k][key] * (1-p)**k, np.mean(counts), len(counts)))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
